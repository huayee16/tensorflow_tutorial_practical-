{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNN_LSTM_2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "fMNJyKMKPrJj"
      },
      "source": [
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0N-TP9eLPxhB"
      },
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SGDiKXv2PysP",
        "outputId": "1817a4ed-43ea-47c0-e7db-b86ac1724a56"
      },
      "source": [
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "print(f'Length of text: {len(text)} characters')\n",
        "print(text[:250])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of text: 1115394 characters\n",
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y3y8j46pP59s",
        "outputId": "7712405b-b524-4fc0-b538-2faaa70b9d85"
      },
      "source": [
        "vocab = sorted(set(text))\n",
        "print(f'{len(vocab)} unique characters')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "65 unique characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3MVsHjQP8mD"
      },
      "source": [
        "chars2id={char:index for index, char in enumerate(vocab)}\n",
        "  \n",
        "id2chars=np.array(vocab)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dngp1rpbP_Of"
      },
      "source": [
        "def tesx2int(text):\n",
        "  return np.array([chars2id[char] for char in text])\n",
        "def int2text(_int):\n",
        "  return ''.join(id2chars[_int])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QVJkx7-4QBIR"
      },
      "source": [
        "alltext2int = tesx2int(text)\n",
        "alltext2int_dataset = tf.data.Dataset.from_tensor_slices(alltext2int)\n",
        "seq_length = 100\n",
        "per_epoch = len(text)//(seq_length+1)\n",
        "sequences = alltext2int_dataset.batch(seq_length+1, drop_remainder=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cs9eveN3QC2F"
      },
      "source": [
        "def split_input_next(sequence):\n",
        "  input_text = sequence[:-1]#x\n",
        "  next_text = sequence[1:]#y\n",
        "  return input_text, next_text\n",
        "train_dataset = sequences.map(split_input_next)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QYIk6YZ3QKf-",
        "outputId": "8b5876bb-b7af-4928-c77c-9344d252361d"
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "seed = 10000\n",
        "dataset = train_dataset.shuffle(seed).batch(BATCH_SIZE, drop_remainder=True).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "dataset"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PrefetchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oKeun_rSQOOq"
      },
      "source": [
        "vocab_size = len(vocab)\n",
        "embedding_dim = 256\n",
        "rnn_units = 1024\n",
        "batch_size = BATCH_SIZE\n",
        "\n",
        "class load_model(tf.keras.Model):\n",
        "  def __init__(self,vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__(self)\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size,embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(rnn_units,return_sequences=True,return_state=True)\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "  def call(self,x,state=None,return_state=False,training=False):\n",
        "    x = self.embedding(x,training=training)\n",
        "    if state is None:\n",
        "      state = self.gru.get_initial_state(x)\n",
        "    x, state = self.gru(x,initial_state=state,training=training)\n",
        "    x = self.dense(x,training=training)\n",
        "    if return_state:\n",
        "      return x, state\n",
        "    else:\n",
        "      return x\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Imr4Oj6uQabp"
      },
      "source": [
        "model = load_model(vocab_size,embedding_dim,rnn_units)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4U9XmfWbYx1j",
        "outputId": "07293317-dfa2-498c-9db8-9220f7ead478"
      },
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 100, 65) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NJd0rBAJYv4R",
        "outputId": "9a5ee5d0-ee4c-4d85-8e70-8359b4ba2eed"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"load_model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        multiple                  16640     \n",
            "_________________________________________________________________\n",
            "gru (GRU)                    multiple                  3938304   \n",
            "_________________________________________________________________\n",
            "dense (Dense)                multiple                  66625     \n",
            "=================================================================\n",
            "Total params: 4,021,569\n",
            "Trainable params: 4,021,569\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m-1X8QrAQehl",
        "outputId": "7bbec3ee-8940-46ad-c3cb-6ef2e0070c69"
      },
      "source": [
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)#非sotmax,預測結果非[0,1]from_logits設為True\n",
        "model.compile(optimizer='adam', loss=loss)\n",
        "EPOCHS = 20\n",
        "history = model.fit(dataset, epochs=EPOCHS)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "172/172 [==============================] - 25s 122ms/step - loss: 2.7148\n",
            "Epoch 2/20\n",
            "172/172 [==============================] - 23s 123ms/step - loss: 1.9850\n",
            "Epoch 3/20\n",
            "172/172 [==============================] - 23s 123ms/step - loss: 1.7134\n",
            "Epoch 4/20\n",
            "172/172 [==============================] - 23s 123ms/step - loss: 1.5526\n",
            "Epoch 5/20\n",
            "172/172 [==============================] - 23s 123ms/step - loss: 1.4534\n",
            "Epoch 6/20\n",
            "172/172 [==============================] - 23s 124ms/step - loss: 1.3852\n",
            "Epoch 7/20\n",
            "172/172 [==============================] - 23s 123ms/step - loss: 1.3328\n",
            "Epoch 8/20\n",
            "172/172 [==============================] - 23s 124ms/step - loss: 1.2879\n",
            "Epoch 9/20\n",
            "172/172 [==============================] - 23s 123ms/step - loss: 1.2470\n",
            "Epoch 10/20\n",
            "172/172 [==============================] - 23s 124ms/step - loss: 1.2073\n",
            "Epoch 11/20\n",
            "172/172 [==============================] - 23s 123ms/step - loss: 1.1674\n",
            "Epoch 12/20\n",
            "172/172 [==============================] - 23s 124ms/step - loss: 1.1265\n",
            "Epoch 13/20\n",
            "172/172 [==============================] - 23s 123ms/step - loss: 1.0828\n",
            "Epoch 14/20\n",
            "172/172 [==============================] - 23s 123ms/step - loss: 1.0382\n",
            "Epoch 15/20\n",
            "172/172 [==============================] - 23s 123ms/step - loss: 0.9891\n",
            "Epoch 16/20\n",
            "172/172 [==============================] - 23s 123ms/step - loss: 0.9396\n",
            "Epoch 17/20\n",
            "172/172 [==============================] - 23s 122ms/step - loss: 0.8868\n",
            "Epoch 18/20\n",
            "172/172 [==============================] - 23s 123ms/step - loss: 0.8337\n",
            "Epoch 19/20\n",
            "172/172 [==============================] - 22s 122ms/step - loss: 0.7819\n",
            "Epoch 20/20\n",
            "172/172 [==============================] - 23s 123ms/step - loss: 0.7327\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-o9Q0tetQiyB"
      },
      "source": [
        "def generate_text(model,input_word,state=None):\n",
        "  num = 1000\n",
        "  temperature = 1.0 #\n",
        "  input_id = tesx2int(input_word)\n",
        "  input_id = tf.expand_dims(input_id, 0)\n",
        "  result = []\n",
        "\n",
        "  model.reset_states()\n",
        "  for i in range(num):\n",
        "    predit,state = model(input_id,state=state,return_state=True)\n",
        "    predit = predit[:,-1,:]\n",
        "    predit = predit / temperature\n",
        "    predit_id = tf.random.categorical(predit, num_samples=1)\n",
        "    input_id = predit_id\n",
        "    #input_id = tf.expand_dims(predit_id, 0)\n",
        "    predit_id = tf.squeeze(predit_id, axis=-1).numpy()\n",
        "    predit_id = int2text(predit_id)\n",
        "    result.append(predit_id)\n",
        "  \n",
        "  return print(input_word +'\\n'+ ''.join(result))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QCt9MXTvRmSV",
        "outputId": "80447d63-11e3-4eb2-cef5-4cd2ca7901af"
      },
      "source": [
        "generate_text(model, input_word='ROMEO: ')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROMEO: \n",
            "have you done, so many hours; the\n",
            "punusant of you all, dies doom the crown.\n",
            "\n",
            "YORK:\n",
            "Hing Liesing he is good to be obedoin.\n",
            "What, like a mighty sea?\n",
            "\n",
            "STANLEY:\n",
            "I may not weer; for I'll to go to cry again.\n",
            "Did you behold your daughter's heir,\n",
            "To make commits them a lover's gentle Claudio.\n",
            "\n",
            "JULIET:\n",
            "O, no more than let here be furnish'd by thy hand;\n",
            "And so, my most ofet them!\n",
            "\n",
            "LUCIO:\n",
            "For all at fretch, which was most quick at the night\n",
            "That I not have a brother still.\n",
            "\n",
            "GLOUCESTER:\n",
            "What, no matter, of his own sovereign,\n",
            "You do between your lordship; but out air\n",
            "The dangerous triumphs what mine ears a little winter'd\n",
            "Rupping of this feast, to kind earth\n",
            "With one that have before wenches from this king.\n",
            "\n",
            "GLOUCESTER:\n",
            "The general make hour begin to cry From contemn'd\n",
            "Shall be the Lady Bona traitors! free\n",
            "pinch'd twenty heads and before me a lamentard word dispersed\n",
            "The one in the heart of the fire.\n",
            "\n",
            "SICINIUS:\n",
            "He is a brave bed!\n",
            "\n",
            "HASTINGS:\n",
            "Killer? now lords, begins our parting seas.\n",
            "\n",
            "QUEEN MARGARE\n"
          ]
        }
      ]
    }
  ]
}